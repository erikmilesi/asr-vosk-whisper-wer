# Vosk vs Whisper â€” Benchmark com WER (PortfÃ³lio do Projeto de InteligÃªncia de ComunicaÃ§Ãµes)

Este repositÃ³rio demonstra, de forma **aberta e reprodutÃ­vel**, a comparaÃ§Ã£o entre os motores de ASR **Vosk** e **Whisper** usando a mÃ©trica **WER (Word Error Rate)** em amostras pÃºblicas.  
Ele faz parte do meu storytelling tÃ©cnico sobre o desenvolvimento de um **sistema de InteligÃªncia de ComunicaÃ§Ãµes** composto por: **reduÃ§Ã£o de ruÃ­do â†’ transcriÃ§Ã£o de fala â†’ detecÃ§Ã£o de palavrasâ€‘chave**.

> âš ï¸ **Nota importante:** O **core proprietÃ¡rio** do meu pipeline (filtros de ruÃ­do, KWS avanÃ§ado, otimizaÃ§Ãµes) nÃ£o estÃ¡ neste repositÃ³rio por razÃµes de **patente**. Aqui vocÃª encontra apenas o **benchmark de transcriÃ§Ã£o** com dados pÃºblicos.

---

## ğŸ§± Estrutura
```text
asr-vosk-whisper-wer/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ scripts/
â”‚  â”œâ”€ get_models.py
â”‚  â”œâ”€ prepare_dataset.py
â”‚  â”œâ”€ prepare_librispeech_full.py
â”‚  â”œâ”€ benchmark_dataset_en.py
â”‚  â””â”€ show_stats.py
â”œâ”€ models/               # (baixados via get_models.py)
â”œâ”€ samples/
â”‚  â”œâ”€ en/                # (LibriSpeech)
â””â”€ results/
   â””â”€ benchmark_en.csv   # (gerado)
```

---

## âš™ï¸ 1) Ambiente
```bash
conda create -n asr python=3.10 -y
conda activate asr
pip install -r requirements.txt
```

- As dependÃªncias principais incluem `vosk`, `openai-whisper`, `torch`, `jiwer`, `datasets`, `librosa`, `pandas` e `matplotlib`. Veja `requirements.txt` para a lista completa.

## â¬‡ï¸ 2) Baixar modelos Vosk
```bash
python scripts/get_models.py --lang en
```
- Os scripts criam pastas em `./models/` (ex.: `models/vosk-en`, `models/vosk-pt`).  
- Se o link padrÃ£o mudar, use `--url` para informar manualmente a URL do modelo Vosk.

## ğŸ§ 3) Preparar dataset de teste leve (EN)
```bash
python scripts\prepare_librispeech_full.py --split test --max_items 50
```
- Baixa poucas amostras por **streaming**: `LibriSpeech` (EN).
- Gera atÃ© 50 pares WAV+TXT (normalizados para 16 kHz mono) em `samples/en`.
- ParÃ¢metros Ãºteis: `--overwrite skip|force`, `--reset` para limpar saÃ­das.

## ğŸ§ª 4) Rodar benchmark (EN)
```bash
python scripts/benchmark_dataset_en.py
```
- Gera `results/benchmark_en.csv` com **engine, arquivo, tempo (s), WER**.

## ğŸ“Š 5) EstatÃ­sticas rÃ¡pidas
```bash
python scripts/show_stats.py
```
SaÃ­da esperada (exemplo):
```text
=== EstatÃ­sticas de Benchmark ===

â†’ Amostras por engine:
whisper-base    50
vosk            50

â†’ Tempo mÃ©dio por engine (s):
engine
vosk            0.221
whisper-base    0.974

â†’ WER mÃ©dio por engine:
engine
vosk            0.24
whisper-base    0.11
```

---

## ğŸ§­ Roadmap (o que vem por aÃ­)
- Comparativo PTâ€‘BR (Common Voice) e anÃ¡lise de sotaques regionais;
- Tradeâ€‘off **latÃªncia Ã— precisÃ£o** e fator de tempo real (RTF);
- IntegraÃ§Ã£o de **reduÃ§Ã£o de ruÃ­do** antes da transcriÃ§Ã£o;
- DetecÃ§Ã£o de **palavrasâ€‘chave** (KWS) em pipeline.

---

## ğŸ¤ ContribuiÃ§Ãµes
- *Pull requests* sÃ£o bemâ€‘vindos para melhorar reprodutibilidade e benchmark de WER.
- O pipeline proprietÃ¡rio (ruÃ­do/KWS) nÃ£o serÃ¡ aberto nesta fase.

## ğŸ“„ LicenÃ§a
- Sem licenÃ§a aberta neste momento. ConteÃºdo disponibilizado apenas para fins de **demonstraÃ§Ã£o de benchmark**.
